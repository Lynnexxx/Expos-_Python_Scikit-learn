{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e923ee6",
   "metadata": {},
   "source": [
    "# <center> <b> <span style=\"color:orange;\"> Introduction à Scikit learn </span> </b></center>\n",
    "\n",
    "### <center> <b> <span style=\"color:green;\">Projet de priorité 2 </span> </b></center>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### <left> <b> <span style=\"color:brown;\">Présenté par : </span> </b></left>\n",
    "### <left> <b> <span style=\"color:brown;\"> </span> </b></left>Murielle AGBAHOLOU \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b3e64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Scikit-learn est une bibliothèque libre Python destinée à l'apprentissage automatique.\n",
    "Vitale, elle utilisée pour créer des modèles statistiques permettant de faire des prédictions.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085b1379",
   "metadata": {},
   "source": [
    "<b> Conditions préalables<b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3179d1d",
   "metadata": {},
   "source": [
    "<p> Pour l'utiliser, l'utilisateur doit comprendre au préalable les bibilothèques de base <b> numpy, matplotlib, scipy, pandas et sympy</b>, les installer,et chercher à comprendre également l'apprentissage automatique et les algorithmes d'apprentissage automatique\n",
    "    </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15505bc",
   "metadata": {},
   "source": [
    "Pour accéder à une explication claire de l'apprentissage automatique, voir [cette page](https://fr.wikipedia.org/wiki/Apprentissage_automatique).\n",
    "L'on pourra aussi consulter [cette page](http://www.tutorialspoint.com/python/python_basic_operators.htm) pour "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1bd403",
   "metadata": {},
   "source": [
    "<b>Installation des bibliothèques annexes et de la bibliothèque Scikit-Learn (communément appelée sklearn)<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a26ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install numpy\n",
    "pip install scipy\n",
    "pip install matplotlib\n",
    "pip install pandas\n",
    "pip install sympy\n",
    "\n",
    "Après avoir installé ces modules, faire:\n",
    "pip install -U scikit-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1cd39a",
   "metadata": {},
   "source": [
    "<b>Ensemble d'entraînement et ensemble de test</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f813d75",
   "metadata": {},
   "source": [
    "<p>\n",
    "\n",
    "<b>L'apprentissage automatique </b> consiste à apprendre certaines propriétés d'un ensemble de données, puis à tester ces propriétés par rapport à un autre ensemble de données.\n",
    "Une pratique courante en apprentissage automatique consiste à évaluer un algorithme en divisant un ensemble de données en deux.\n",
    "L'un de ces ensembles est <b> l' ensemble d'apprentissage </b>, sur lequel nous apprenons certaines propriétés, l'autre ensemble est <b>l'ensemble de test</b>, sur lequel seront testées les propriétés apprises.\n",
    "    \n",
    "  Il est nécessaire que tous les ensembles que l'on manipule soient représentatifs du cas à modéliser sans quoi,  le modèle sera testé sur des données qui ne correspondent pas aux données d'entraînement. C'est pourquoi on doit s'assurer que les distributions de tous ces ensembles sont comparables. Généralement on réalise des coupes aléatoires dans le dataset pour déterminer les sous-ensembles.\n",
    "\n",
    "\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefbedb7",
   "metadata": {},
   "source": [
    "<b>Chargement d'un exemple de jeu de données </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40aa2b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "scikit-learn est livré avec quelques ensembles de données standard, par exemple les ensembles de données sur l' iris et les chiffres pour la classification et l' ensemble de données sur le diabète pour la régression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f3533b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Un ensemble de données est un objet de type dictionnaire qui contient toutes les données et certaines métadonnées sur les données.\n",
    "Ces données sont stockées dans le .datamembre, qui est un tableau. \n",
    "Dans le cas d'un problème supervisé, une ou plusieurs variables de réponse sont stockées dans le membre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4543a0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "Le dataset IRIS contient 150 observations de fleurs d'iris, avec pour chacune d'elles quatre caractéristiques (longueur et largeur des pétales, longueur et largeur des sépales) et le nom de la variété d'iris. Le cas d'usage habituel est de créer des modèles permettant de prédire la variété en fonction des caractéristiques observées. De cette manière, les caractéristiques sont considérées comme des features du dataset (habituellement notées X) et la variété est la target (l'élément à deviner, habituellement noté Y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461ac8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Le découpage d'un dataset en ensemble d'entraînement et ensemble de test se fait par Scikit-learn :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c1875f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# L'ensemble de test aura 20 % des éléments de départ. \n",
    "# L'ensemble d'entrainement contiendra les 80 % restant.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7cd0cf",
   "metadata": {},
   "source": [
    "<b>Validation croisée<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5c380c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Le souci dans l'utilisation d'un ensemble de test c'est que l'on réduit l'ensemble d'entraînement pour isoler un ensemble de test.\n",
    "Si le jeu de données est restreint, cela revient à le restreindre encore plus et cela pourra pénaliser l'apprentissage : avec trop peu de données, le modèle ne pourra pas apprendre correctement.\n",
    "\n",
    "Un autre souci à l'utilisation d'un ensemble de test c'est qu'il est nécessairement plus petit et que les mesures de performances du modèle sur ce petit ensemble risquent de ne pas représenter sa capacité à généraliser. \n",
    "Un ensemble petit aura plus de mal à rester représentatif des données réelles.\n",
    "\n",
    "C'est pourquoi on choisira souvent d'utiliser une validation croisée pour affiner les hyper-paramètres d'un modèle. La validation croisée consiste en une suite de découpages distincts en un ensemble d'entraînement et un ensemble de validation. Chaque découpage permet de mesurer une performance du modèle et la moyenne des performances de chaque découpage permet de trouver les meilleurs hyper-paramètres sans jamais avoir validé le modèle sur une donnée déjà apprise.\n",
    "\n",
    "\n",
    "Scikit-learn implémente déjà ce mécanisme de découpages :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc1cda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "model=\n",
    "scores = cross_val_score(model, X, Y, cv=5) #cv est le nombre de découpages à réaliser\n",
    "score = scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3498fc1",
   "metadata": {},
   "source": [
    "<b>Classification<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b267335",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Scikit-learn implémente de nombreux algorithmes de classification parmi lesquels :\n",
    "\n",
    "-perceptron multicouches (réseau de neurones) sklearn.neural_network.MLPClassifier ;\n",
    "-machines à vecteurs de support (SVM) sklearn.svm.SVC ;\n",
    "-k plus proches voisins (KNN) sklearn.neighbors.KNeighborsClassifier ;\n",
    "…\n",
    "Ces algorithmes  s'utilise de la même manière, avec la même syntaxe. Vous pouvez ainsi aisément les interchanger, les remplacer et les comparer.\n",
    "\n",
    "Le principe de Scikit-learn est à chaque fois le même :\n",
    "\n",
    "1-instanciation du modèle model = nom_du_modèle(liste des hyper-paramètres) ;\n",
    "2-entraînement du modèle sur les features X et les targets Y : model.fit(X, Y). À la fin de l'entraînement, le modèle est prêt à être utilisé pour prédire des targets ;\n",
    "3-utilisation du modèle pour prédire de nouvelles données : Y_predicted = model.predict(X_to_predict) ;\n",
    "4-mesure de l'efficacité du modèle : model.score(X_test, Y_réels). Cet appel va prédire des Y associés aux X d'entrée et les comparer aux Y_réels.\n",
    "La classification KNN appliquée au dataset IRIS pourrait ressembler à ça :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dbca08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25,random_state=11)\n",
    "knn = KNeighborsClassifier(n_neighbors=10)\n",
    "digit_knn=knn.fit(X_train, y_train)\n",
    "# Estimation de l’erreur de prévision sur l’échantillon test\n",
    "digit_knn.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26e7711",
   "metadata": {},
   "outputs": [],
   "source": [
    "Par chance ce modèle KNN est parfaitement convergent sur ce petit dataset IRIS, mais ce n'est évidemment pas toujours le cas.\n",
    "La matrice de confusion est idéale : elle est diagonale, ce qui signifie que tous les éléments d'une classe ont été prédits dans la même classe.\n",
    "\n",
    "\n",
    "Évidemment, chaque algorithme a ses propres hyper-paramètres.\n",
    "Il y a une valeur par défaut pour chacun, mais il est recommandé de connaître les principaux hyper-paramètres de chaque algorithme : le KNN demande de fournir le nombre de voisins k, le perceptron multicouches demande d'indiquer le nombre de couches et le nombre de neurones de chaque couche"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bbbb3a",
   "metadata": {},
   "source": [
    "<b>Recherche des meilleurs hyper-paramètres<b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ab8ad9",
   "metadata": {},
   "source": [
    "À part avec beaucoup d'intuition, il n'est pas possible de deviner les meilleurs hyper-paramètres pour un ensemble d'entraînement donné. Scikit-learn propose deux méthodes de recherche :\n",
    "\n",
    "<p>1-<b>GridSearchCV</b> : qui va tester le modèle avec toutes les combinaisons d'hyper-paramètres, pour trouver les meilleures valeurs ;</p>\n",
    "2-<b>RandomizedSearchCV</b>, qui va tester au hasard quelques combinaisons d'hyper-paramètres, pour trouver les meilleures.\n",
    "Le principe est de définir un dictionnaire de paramètres et de valeurs à tester :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9e58f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25,random_state=11)\n",
    "# grille de valeurs\n",
    "param=[{\"n_neighbors\":list(range(1,15))}]\n",
    "knn= GridSearchCV(KNeighborsClassifier(),param,cv=5,n_jobs=-1)\n",
    "digit_knn=knn.fit(X_train, y_train)\n",
    "# paramètre optimal\n",
    "digit_knn.best_params_[\"n_neighbors\"]\n",
    "#Le modèle est estimé avec la valeur \"optimale\" du paramètre.\n",
    "knn = KNeighborsClassifier(n_neighbors=digit_knn.best_params_[\"n_neighbors\"])\n",
    "digit_knn=knn.fit(X_train, y_train)\n",
    "\n",
    "digit_knn.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55010fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dans notre exemple, le GridSearchCV va tester le résultat sur les quatre valeurs de n_neighbors fournies.\n",
    "Pour les algorithmes possédant beaucoup d'hyper-paramètres avec beaucoup de possibilités pour chacun, \n",
    "la recherche exhaustive peut prendre du temps et il faudra trouver un meilleur moyen d'optimiser les hyper-paramètres : \n",
    "c'est l'intérêt de RandomizedSearchCV où seul un certain nombre de possibilités aléatoires est testé.\n",
    "\n",
    "Notons l'existence d'une troisième méthode de recherche de meilleurs hyper-paramètres, bien qu'elle ne soit pas officiellement intégrée à Scikit-learn : la recherche génétique. En réalisant des mutations et des croisements sur plusieurs combinaisons d'hyper-paramètres, l'algorithme génétique va essayer de se rapprocher de l'optimum sans explorer toutes les possibilités."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b545f52",
   "metadata": {},
   "source": [
    "<b>Régression<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29d3a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "D'une manière générale, le mécanisme de régression permet d'exhiber une fonction mathématique s'approchant d'un ensemble de données.\n",
    "En étudiant une nouvelle donnée, on pourra la soumettre à la fonction et obtenir la prédiction du modèle.\n",
    "C'est un mécanisme qu'on peut quali²fier de machine learning bien qu'il ne s'agisse pas d'associer une donnée à un ensemble fini de classes de sortie.\n",
    "Les valeurs de sortie sont infinies, mais le traitement se base néanmoins sur un apprentissage.\n",
    "\n",
    "Scikit-learn propose plusieurs méthodes de régression, utilisant des propriétés statistiques des datasets ou jouant sur les métriques utilisées.\n",
    "Les méthodes principalement utilisées sont les régressions linéaires. \n",
    "Bien souvent une partie du préprocessing sera de rendre vos données linéaires, en les transformant.\n",
    "Une fois transformées vous pouvez utiliser les régressions proposées.\n",
    "\n",
    "Par exemple si vous avez des données de forme polynomiale, vous pouvez d'abord appliquer une transformation polynomiale, puis appliquer une régression linéaire. \n",
    "Scikit-learn propose la notion de pipeline pour chainer des opérations. Voici un exemple de régression polynomiale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88f72f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "degre = 2\n",
    "\n",
    "# création d'un ensemble X,Y clairement associé à une parabole\n",
    "X = np.linspace(1, 10, 100)  # création de 100 points en abscisse\n",
    "Y = X**2 + 10*np.random.rand((100))   # création de 100 points en ordonnée\n",
    "\n",
    "X = X.reshape((100, 1))\n",
    "Y = Y.reshape((100, 1))\n",
    "    \n",
    "model = make_pipeline(PolynomialFeatures(degre), Ridge())\n",
    "model.fit(X, Y)\n",
    "Y_pred = model.predict(X)\n",
    "\n",
    "# graphique du résultat\n",
    "plt.scatter(X, Y, c=\"r\")\n",
    "plt.plot(X, Y_pred, c=\"b\", lw=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eaca5dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
